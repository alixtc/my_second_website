---
title: Hello world of neural networks
author: ATC
date: '2021-03-25'
slug: hello-world-of-neural-networks
categories: []
tags: []
---
Below is a very simple example of the typical analysis done with keras as a demonstration of the use of a neural network. It starts as usual by loading the necessary packages in this case only the `keras` package will be necessary. The data correspond to hand-written digits whose images have been scanned and preprocessed so that the actual drawing is centered in a 28 * 28 pixels image, with the corresponding digits specified in the label `y` part of the `df` dataset.
```{r}
library(keras)
df <- dataset_mnist()
c(c(train_df, train_labels), c(test_df, test_labels)) %<-% df
```
The `dataset_mnist()` function allow to directly download the dataset in a conveniently preformated way that allow direct use inside a NN. The third line allows multiple assignement to the specified datasets of the variables stored in the df which is a list containing the data and the labels for the training and testing set.

One very important step regarding the labels which are stored in a column vector, which is impractical for classification, they have to be one hot encoded, meaning a column will be assign to each possible category and for each observatin (each row) the column corresponding to the actual label will contain the value 1, while the 9 other columns will contain the value 0. 
```{r}
train_labels <- to_categorical(train_labels)
test_labels <- to_categorical(test_labels)
```

From then we don't have any more preprocessing steps to perform on the data, we only need to define the model:
```{r}
model <- keras_model_sequential() %>% 
  layer_flatten(input_shape = c(28, 28)) %>% 
  layer_dense(units = 256, activation = 'relu') %>% 
  layer_dropout(rate = 0.3) %>% 
  layer_dense(units = 128, activation = 'relu') %>% 
  layer_dropout(rate = 0.3) %>% 
  layer_dense(units = 64, activation = 'relu') %>% 
  layer_dense(units = 10, activation = 'softmax')
```
The first function is the initiation of neural network using the default built-in keras interface. It first requires to specify the inputs which is done throught the `input_shape = c(28, 28)`, corresponding to the shape of the actual input tensor. This is done inside a `layer_flatten` functions that simply changes the shape of the input into a linear format : a linear 784 input corresponding to (28*28).

Each following step is a sequece of either:

  - A dense layer composed of units (or neurons): 
    * Each neuron will perform the linear combination of the inputs and output a value based on an activation function.
    * The activation function is here the "ReLU" function, one of the most popular types of activation function.
  - or a dropout layer that is a slightly more advance concept that prevents the whole network to overfit, by randomly turning some of the outputs of each layer to 0, at a rate here of 0.3 

We note that the last layer has as many units as there are labels in the data, each of the units will output a probability for a given class. The last layer has a different activation function 'softmax', that will sum all 10 probabilities to 1.


We then need to specify how the NN will be trained:
```{r}
model %>% 
  compile(
    optimizer = 'adam',
    loss = 'categorical_crossentropy',
    metrics = 'accuracy'
  )
```

The loss indicates which type of function should be used to perform the classification. In order to estimate the validity of the model for each prediction from the model we will evaluate how precise it is through the accuracy of each prediction: does the predicted maximum probability correspond to the actual label of the input? The way to update the NN is determined by the optimizer function, here the 'adam' optimizer which also constitute a good defautlt choice for a simple neural network such as that.

We are now ready to train the model on the data, for that we use the `fit()` function.

```{r}
history <- model %>% 
  fit(x = train_df,
      y= train_labels,
      epochs = 20,
      batch_size = 200,
      validation_split = 0.15)
```

The data is passed through the model in batches of 200 samples at a time before leading to optimization of the model. The number of epochs corresponding to 20, indicates that the model will be trained successively 20 times on the data set. And finally the `validation_split` argument indicates that the last 15% of the training data will be set aside and used for evaluation of model performance during the training. In that way the model is evaluated on a different dataset from its training, which is a good practice to ensure that the model will be able to generalize its predictions to unseen data.

```{r}
plot(history)
```
Here with the plot of the information gathered during the training we see that the model is not overfitting as the curves in training phase and validation phase do not cross indicating that the model is not excessively adapting to the training dataset.

Finally we evaluate the trained model on the testing set.

```{r}
model %>% evaluate(test_df, test_labels)
```

We get an accuracy around 97% which is great for a relatively simple network. If we were to use more appropriate feature, mainly convolutional layer which are infinitely more suited to process images, we would get an even better result.