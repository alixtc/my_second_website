---
title: Spam text identification
author: ATC
date: '2021-05-05'
slug: spam-text-identification
categories: []
tags: []
---

<script src="{{< blogdown/postref >}}index_files/header-attrs/header-attrs.js"></script>


<p>The following example is simple attempt to classify received text messages as either “spam” or valid text “ham”. This example is interesting as it deals with some simple concepts of machine learning but also some processing for natural languages. This post has been mainly inspired by a chapter in the book “Machine Learning with R” by Brett Lantz, with a slightly different approach by relying mostly on Tidymodels and Tidyverse syntax.</p>
<p>The purpose is to use a naive Bayes classifier to determine whether or not the incoming message should be allowed.</p>
<p>First we will load the main packages to handle the data.</p>
<pre class="r"><code>library(RCurl)
library(tidyverse)
library(tidymodels)</code></pre>
<p>Let’s download the data from the publisher repository and have a brief look at the data.</p>
<pre class="r"><code>sms_raw &lt;- getURL(&quot;https://raw.githubusercontent.com/PacktPublishing/Machine-Learning-with-R-Third-Edition/master/Chapter04/sms_spam.csv&quot;)
sms_raw &lt;- read_csv(sms_raw)
head(sms_raw)</code></pre>
<pre><code>## # A tibble: 6 x 2
##   type  text                                                                    
##   &lt;chr&gt; &lt;chr&gt;                                                                   
## 1 ham   Hope you are having a good week. Just checking in                       
## 2 ham   K..give back my thanks.                                                 
## 3 ham   Am also doing in cbe only. But have to pay.                             
## 4 spam  complimentary 4 STAR Ibiza Holiday or £10,000 cash needs your URGENT co…
## 5 spam  okmail: Dear Dave this is your final notice to collect your 4* Tenerife…
## 6 ham   Aiya we discuss later lar... Pick u up at 4 is it?</code></pre>
<p>We have 2 columns, one called <code>type</code> containing the classification of the text message “ham”/“spam” and the second <code>text</code> containing the text message itself. We will first convert the first column to a factor. In a second time we split the data between into training and testing sets.</p>
<pre class="r"><code>sms_raw$type &lt;- factor(sms_raw$type)

set.seed(123)
split &lt;- initial_split(sms_raw, prop = 0.8, strata = &quot;type&quot;)
nb_train_sms &lt;- training(split)
nb_test_sms &lt;- testing(split)</code></pre>
<p>Because the text data contains unnecessary information, such as punctuation marks (.;:,…), stop words (the, is, at, which,…) they have to be removed from the analysis. Similarly, many numbers also appear only once as they are phone number, and thus are not useful for the classifier to identify patterns and generalizing feature, so they will be removed as well.</p>
<p>First let’s load the packages for text processing.</p>
<pre class="r"><code>library(textrecipes)
library(tm) 
library(discrim)</code></pre>
<p>Note: The loaded packages contains many functions that allow the user to deal easily with this kind of issues. Although <code>tm</code> and <code>discrim</code> packages can be used on their own with base R as describe in the chapter 4 of “Machine Learning with R”. We will use them inside a tidymodel workflow to mimic the preprocessing steps but they are some equivalent functions inside the <code>textrecipes</code> package.</p>
<pre class="r"><code># Text preprocessing
reci_sms &lt;- 
  recipe(type ~.,
         data = nb_train_sms) %&gt;% 
  step_mutate(text = str_to_lower(text)) %&gt;% 
  step_mutate(text = removeNumbers(text)) %&gt;% 
  step_mutate(text = removePunctuation(text)) %&gt;% 
  step_tokenize(text) %&gt;% 
  step_stopwords(text, custom_stopword_source = stopwords()) %&gt;% 
  step_stem(text) %&gt;% 
  step_tokenfilter(text, min_times = 6, max_tokens = 1500) %&gt;% 
  step_tf(text, weight_scheme = &quot;binary&quot;)  %&gt;% 
  step_mutate_at(contains(&quot;tf&quot;), fn = function(x){ifelse(x == TRUE, &quot;Yes&quot;, &quot;No&quot;)})</code></pre>
<p>The first 3 steps of the recipes are here to switch all texts messages to lower case and remove numbers and punctuation marks as announced earlier.</p>
<p>Then we start using some of the functions of <code>textrecipes</code>. The first one <code>step_tokenize</code> consists in initializing the tokenisation of the text, meaning splitting it into smaller units that can be analyzed easily. Here the tokens are words but they could be letters, group of words, syllabs… This is followed by a step to remove all stop words from the text message as they have little to no meaningful value.</p>
<p><code>step_stem()</code>converts the token i.e. the words that are derived from a common origin to a simpler form, for example “learned”, “learns” are simplified in “learn”.</p>
<p>The filtering of tokens keeps only the words that appear at least 6 times inside the whole corpus. This is done to remove occurences of words that do not appear ofter enough to yield any potential to identify general classification rules.</p>
<p>Finally the data are converted to a long format with <code>step_tf</code> where each column corresponds to a token and if the word appear in a given text message the corresponding value will be set to 1. If the word does not appear, the value will be 0.</p>
<p>Ultimately the last line converts these 0 and 1 to “No” and “Yes” respectively as Naive Bayes classifier deals more easily with factors rather than numbers.</p>
<p>The data end up looking like this, a dataframe with a lot of zeros and some sparse ones.</p>
<pre class="r"><code>head(juice(prep(reci_sms)))</code></pre>
<pre><code>## Warning: max_features was set to &#39;1500&#39;, but only 1113 was available and
## selected.</code></pre>
<pre><code>## # A tibble: 6 x 1,114
##   type  tf_text_ tf_text_abiola tf_text_abl tf_text_abt tf_text_accept
##   &lt;fct&gt; &lt;fct&gt;    &lt;fct&gt;          &lt;fct&gt;       &lt;fct&gt;       &lt;fct&gt;         
## 1 ham   No       No             No          No          No            
## 2 ham   No       No             No          No          No            
## 3 ham   No       No             No          No          No            
## 4 ham   No       No             No          No          No            
## 5 ham   No       No             No          No          No            
## 6 ham   No       No             No          No          No            
## # … with 1,108 more variables: tf_text_access &lt;fct&gt;, tf_text_account &lt;fct&gt;,
## #   tf_text_across &lt;fct&gt;, tf_text_activ &lt;fct&gt;, tf_text_actual &lt;fct&gt;,
## #   tf_text_ad &lt;fct&gt;, tf_text_add &lt;fct&gt;, tf_text_address &lt;fct&gt;,
## #   tf_text_admir &lt;fct&gt;, tf_text_advanc &lt;fct&gt;, tf_text_aft &lt;fct&gt;,
## #   tf_text_afternoon &lt;fct&gt;, tf_text_aftr &lt;fct&gt;, tf_text_ag &lt;fct&gt;,
## #   tf_text_ago &lt;fct&gt;, tf_text_ah &lt;fct&gt;, tf_text_aha &lt;fct&gt;,
## #   tf_text_aight &lt;fct&gt;, tf_text_air &lt;fct&gt;, tf_text_alex &lt;fct&gt;,
## #   tf_text_almost &lt;fct&gt;, tf_text_alon &lt;fct&gt;, tf_text_alreadi &lt;fct&gt;,
## #   tf_text_alright &lt;fct&gt;, tf_text_also &lt;fct&gt;, tf_text_alwai &lt;fct&gt;,
## #   tf_text_amaz &lt;fct&gt;, tf_text_ampm &lt;fct&gt;, tf_text_an &lt;fct&gt;,
## #   tf_text_angri &lt;fct&gt;, tf_text_announc &lt;fct&gt;, tf_text_anoth &lt;fct&gt;,
## #   tf_text_answer &lt;fct&gt;, tf_text_anymor &lt;fct&gt;, tf_text_anyon &lt;fct&gt;,
## #   tf_text_anyth &lt;fct&gt;, tf_text_anytim &lt;fct&gt;, tf_text_anywai &lt;fct&gt;,
## #   tf_text_apart &lt;fct&gt;, tf_text_app &lt;fct&gt;, tf_text_appli &lt;fct&gt;,
## #   tf_text_ar &lt;fct&gt;, tf_text_ard &lt;fct&gt;, tf_text_area &lt;fct&gt;,
## #   tf_text_arent &lt;fct&gt;, tf_text_around &lt;fct&gt;, tf_text_arrang &lt;fct&gt;,
## #   tf_text_arriv &lt;fct&gt;, tf_text_asap &lt;fct&gt;, tf_text_ask &lt;fct&gt;,
## #   tf_text_askd &lt;fct&gt;, tf_text_askin &lt;fct&gt;, tf_text_attempt &lt;fct&gt;,
## #   tf_text_auction &lt;fct&gt;, tf_text_av &lt;fct&gt;, tf_text_avail &lt;fct&gt;,
## #   tf_text_awai &lt;fct&gt;, tf_text_await &lt;fct&gt;, tf_text_awak &lt;fct&gt;,
## #   tf_text_award &lt;fct&gt;, tf_text_awesom &lt;fct&gt;, tf_text_b &lt;fct&gt;,
## #   tf_text_babe &lt;fct&gt;, tf_text_babi &lt;fct&gt;, tf_text_back &lt;fct&gt;,
## #   tf_text_bad &lt;fct&gt;, tf_text_bag &lt;fct&gt;, tf_text_bak &lt;fct&gt;,
## #   tf_text_bank &lt;fct&gt;, tf_text_bare &lt;fct&gt;, tf_text_basic &lt;fct&gt;,
## #   tf_text_bath &lt;fct&gt;, tf_text_bb &lt;fct&gt;, tf_text_bcoz &lt;fct&gt;,
## #   tf_text_bdai &lt;fct&gt;, tf_text_beauti &lt;fct&gt;, tf_text_becom &lt;fct&gt;,
## #   tf_text_bed &lt;fct&gt;, tf_text_bedroom &lt;fct&gt;, tf_text_believ &lt;fct&gt;,
## #   tf_text_best &lt;fct&gt;, tf_text_better &lt;fct&gt;, tf_text_big &lt;fct&gt;,
## #   tf_text_bill &lt;fct&gt;, tf_text_bird &lt;fct&gt;, tf_text_birthdai &lt;fct&gt;,
## #   tf_text_bit &lt;fct&gt;, tf_text_bless &lt;fct&gt;, tf_text_blue &lt;fct&gt;,
## #   tf_text_bluetooth &lt;fct&gt;, tf_text_bodi &lt;fct&gt;, tf_text_boi &lt;fct&gt;,
## #   tf_text_bold &lt;fct&gt;, tf_text_bonu &lt;fct&gt;, tf_text_book &lt;fct&gt;,
## #   tf_text_bore &lt;fct&gt;, tf_text_boss &lt;fct&gt;, tf_text_bother &lt;fct&gt;,
## #   tf_text_bout &lt;fct&gt;, tf_text_box &lt;fct&gt;, …</code></pre>
<p>Finally we prepare the model for a classification using NaiveBayes. The model is first defined then included inside a workflow where the preprocessing steps are defined by the recipe we wrote earlier on. In the end the model is fit on the training dataset.</p>
<pre class="r"><code>nb_model &lt;- naive_Bayes() %&gt;% 
  set_engine(&quot;klaR&quot;) 

nb_fit &lt;- workflow() %&gt;%
  add_model(nb_model) %&gt;%
  add_recipe(reci_sms) %&gt;%
  fit(data = nb_train_sms)</code></pre>
<pre><code>## Warning: max_features was set to &#39;1500&#39;, but only 1113 was available and
## selected.</code></pre>
<p>We can then use the fitted model to predict the nature of text messages in the testing dataset.</p>
<pre class="r"><code>nb_tidy_pred &lt;- nb_fit %&gt;% predict(nb_test_sms)
nb_tidy_pred&lt;- nb_tidy_pred %&gt;% bind_cols(truth = nb_test_sms$type) 
caret::confusionMatrix(nb_tidy_pred$.pred_class, nb_tidy_pred$truth)</code></pre>
<pre><code>## Confusion Matrix and Statistics
## 
##           Reference
## Prediction ham spam
##       ham  958   21
##       spam   5  129
##                                          
##                Accuracy : 0.9766         
##                  95% CI : (0.966, 0.9847)
##     No Information Rate : 0.8652         
##     P-Value [Acc &gt; NIR] : &lt; 2.2e-16      
##                                          
##                   Kappa : 0.8951         
##                                          
##  Mcnemar&#39;s Test P-Value : 0.003264       
##                                          
##             Sensitivity : 0.9948         
##             Specificity : 0.8600         
##          Pos Pred Value : 0.9785         
##          Neg Pred Value : 0.9627         
##              Prevalence : 0.8652         
##          Detection Rate : 0.8607         
##    Detection Prevalence : 0.8796         
##       Balanced Accuracy : 0.9274         
##                                          
##        &#39;Positive&#39; Class : ham            
## </code></pre>
<p>We get an overall accuracy above 95 which is pretty good. Also we have very few good messages beeing incorrectly classified as “spam” which could potentially be troublesome as this might lead the person to ignore potentially important message. But nonetheless it’s still a effective way to discriminate spam text with relatively simple code and very little engineering of the data.</p>
